{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7e7797",
   "metadata": {},
   "source": [
    "# Utility Validation: Comparing Model Performance on Original vs Anonymized Data\n",
    "\n",
    "> **Note:** This notebook has only been tested once and should undergo proper validation before use in production or for drawing conclusions. Treat results as exploratory.\n",
    "\n",
    "This notebook evaluates the utility of anonymized data by comparing machine learning model performance on:\n",
    "1. **Original Data** - Raw customer data\n",
    "2. **Anonymized Data** - Data processed with anonymization techniques\n",
    "\n",
    "We'll predict **income** from **age** and **zip code** using various models, then compare metrics and visualize the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9912cf0",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d40dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import anonymization modules\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.anonymizer import Anonymizer\n",
    "from src.config_loader import load_config\n",
    "from src.utility_metrics import compare_utility\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f04973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data\n",
    "df = pd.read_csv(\"../fixtures/customers.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate age from date of birth\n",
    "df[\"dob\"] = pd.to_datetime(df[\"dob\"])\n",
    "current_year = datetime.now().year\n",
    "df[\"age\"] = current_year - df[\"dob\"].dt.year\n",
    "\n",
    "# Extract first 3 digits of zip code for generalization\n",
    "df[\"zip\"] = df[\"zip\"].astype(str).str.zfill(5)  # Ensure 5 digits\n",
    "df[\"zip_prefix\"] = df[\"zip\"].str[:3].astype(int)\n",
    "\n",
    "# Select features for modeling\n",
    "print(\"Feature Engineering:\")\n",
    "print(f\"  ‚úì Calculated age from DOB\")\n",
    "print(f\"  ‚úì Extracted zip code prefix (first 3 digits)\")\n",
    "print(f\"\\nAge range: {df['age'].min()} - {df['age'].max()}\")\n",
    "print(f\"Income range: ${df['income'].min():,} - ${df['income'].max():,}\")\n",
    "print(f\"Unique zip prefixes: {df['zip_prefix'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e70aa9",
   "metadata": {},
   "source": [
    "## 2. Data Anonymization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load anonymization config\n",
    "config = load_config(\"../config/anonymization_rules.yaml\")\n",
    "\n",
    "# Prepare data for anonymization with column mapping\n",
    "df_to_anonymize = df[[\"age\", \"zip\", \"income\"]].copy()\n",
    "\n",
    "# Create column mapping (map column names to PII types in config)\n",
    "column_mapping = {\"age\": \"age\", \"zip\": \"zipcode\", \"income\": \"income\"}\n",
    "\n",
    "# Apply anonymization\n",
    "anonymizer = Anonymizer(config)\n",
    "df_anonymized = anonymizer.anonymize(df_to_anonymize, column_mapping=column_mapping)\n",
    "\n",
    "print(\"Anonymization Complete!\")\n",
    "print(f\"\\nOriginal Data Sample:\")\n",
    "print(df_to_anonymize.head())\n",
    "print(f\"\\nAnonymized Data Sample:\")\n",
    "print(df_anonymized.head())\n",
    "\n",
    "# Get anonymization statistics\n",
    "stats = anonymizer.get_statistics()\n",
    "print(f\"\\nAnonymization Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b6a7a-4dca-4ff1-8d5c-f605bba5a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse generalized ranges back to numeric for modeling\n",
    "def parse_range_to_numeric(value):\n",
    "    \"\"\"Convert generalized ranges (e.g., '30-39') to numeric (midpoint).\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "\n",
    "    # Already numeric\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "\n",
    "    # Parse range\n",
    "    str_value = str(value).strip()\n",
    "    if \"-\" in str_value:\n",
    "        try:\n",
    "            parts = str_value.split(\"-\")\n",
    "            if len(parts) == 2:\n",
    "                start = float(parts[0].strip())\n",
    "                end = float(parts[1].strip())\n",
    "                return (start + end) / 2.0\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Try direct conversion\n",
    "    try:\n",
    "        return float(str_value)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Process anonymized data for modeling\n",
    "df_anon_processed = df_anonymized.copy()\n",
    "df_anon_processed[\"age\"] = df_anon_processed[\"age\"].apply(parse_range_to_numeric)\n",
    "df_anon_processed[\"zip\"] = df_anon_processed[\"zip\"].apply(parse_range_to_numeric)\n",
    "df_anon_processed[\"income\"] = df_anon_processed[\"income\"].apply(parse_range_to_numeric)\n",
    "\n",
    "# Extract zip prefix from anonymized data\n",
    "df_anon_processed[\"zip_prefix\"] = (df_anon_processed[\"zip\"] / 100).astype(int)\n",
    "\n",
    "print(\"Processed Anonymized Data:\")\n",
    "print(df_anon_processed.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df_anon_processed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53005f98",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare original data\n",
    "X_original = df[[\"age\", \"zip_prefix\"]].copy()\n",
    "y_original = df[\"income\"].copy()\n",
    "\n",
    "# Split original data\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_original, y_original, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original Data Split:\")\n",
    "print(f\"  Training set: {X_train_orig.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test_orig.shape[0]} samples\")\n",
    "print(f\"\\nFeatures: {list(X_original.columns)}\")\n",
    "print(f\"Target: income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare anonymized data\n",
    "X_anon = df_anon_processed[[\"age\", \"zip_prefix\"]].copy()\n",
    "y_anon = df_anon_processed[\"income\"].copy()\n",
    "\n",
    "# Use same split indices for fair comparison\n",
    "X_train_anon = X_anon.iloc[X_train_orig.index]\n",
    "X_test_anon = X_anon.iloc[X_test_orig.index]\n",
    "y_train_anon = y_anon.iloc[y_train_orig.index]\n",
    "y_test_anon = y_anon.iloc[y_test_orig.index]\n",
    "\n",
    "print(\"Anonymized Data Split:\")\n",
    "print(f\"  Training set: {X_train_anon.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test_anon.shape[0]} samples\")\n",
    "print(f\"\\nSample comparison:\")\n",
    "print(\"\\nOriginal:\")\n",
    "print(X_train_orig.head())\n",
    "print(\"\\nAnonymized:\")\n",
    "print(X_train_anon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2a748",
   "metadata": {},
   "source": [
    "## 4. Train Models on Original Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=1.0),\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=10, random_state=42\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(\n",
    "        n_estimators=100, max_depth=5, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Train and evaluate models on original data\n",
    "results_original = {}\n",
    "\n",
    "print(\"Training models on ORIGINAL data...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_orig)\n",
    "    y_pred_test = model.predict(X_test_orig)\n",
    "\n",
    "    # Metrics\n",
    "    train_mae = mean_absolute_error(y_train_orig, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test_orig, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_test))\n",
    "    train_r2 = r2_score(y_train_orig, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_orig, y_pred_test)\n",
    "\n",
    "    results_original[name] = {\n",
    "        \"model\": model,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"predictions\": y_pred_test,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train MAE: ${train_mae:,.2f}\")\n",
    "    print(f\"  Test MAE:  ${test_mae:,.2f}\")\n",
    "    print(f\"  Train RMSE: ${train_rmse:,.2f}\")\n",
    "    print(f\"  Test RMSE:  ${test_rmse:,.2f}\")\n",
    "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  Test R¬≤:  {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì Original data models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0144ed3",
   "metadata": {},
   "source": [
    "## 5. Train Models on Anonymized Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models on anonymized data\n",
    "results_anonymized = {}\n",
    "\n",
    "print(\"Training models on ANONYMIZED data...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model_template in models.items():\n",
    "    # Create new model instance (don't reuse trained model)\n",
    "    if name == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "    elif name == \"Ridge Regression\":\n",
    "        model = Ridge(alpha=1.0)\n",
    "    elif name == \"Lasso Regression\":\n",
    "        model = Lasso(alpha=1.0)\n",
    "    elif name == \"Random Forest\":\n",
    "        model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    elif name == \"Gradient Boosting\":\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators=100, max_depth=5, random_state=42\n",
    "        )\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train_anon, y_train_anon)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_anon)\n",
    "    y_pred_test = model.predict(X_test_anon)\n",
    "\n",
    "    # Metrics\n",
    "    train_mae = mean_absolute_error(y_train_anon, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test_anon, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_anon, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_anon, y_pred_test))\n",
    "    train_r2 = r2_score(y_train_anon, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_anon, y_pred_test)\n",
    "\n",
    "    results_anonymized[name] = {\n",
    "        \"model\": model,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"predictions\": y_pred_test,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train MAE: ${train_mae:,.2f}\")\n",
    "    print(f\"  Test MAE:  ${test_mae:,.2f}\")\n",
    "    print(f\"  Train RMSE: ${train_rmse:,.2f}\")\n",
    "    print(f\"  Test RMSE:  ${test_rmse:,.2f}\")\n",
    "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  Test R¬≤:  {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì Anonymized data models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8babbfe6",
   "metadata": {},
   "source": [
    "## 6. Compare Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ee2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    orig = results_original[model_name]\n",
    "    anon = results_anonymized[model_name]\n",
    "\n",
    "    comparison_data.append(\n",
    "        {\n",
    "            \"Model\": model_name,\n",
    "            \"Original MAE\": orig[\"test_mae\"],\n",
    "            \"Anonymized MAE\": anon[\"test_mae\"],\n",
    "            \"MAE Difference\": anon[\"test_mae\"] - orig[\"test_mae\"],\n",
    "            \"MAE % Change\": ((anon[\"test_mae\"] - orig[\"test_mae\"]) / orig[\"test_mae\"])\n",
    "            * 100,\n",
    "            \"Original R¬≤\": orig[\"test_r2\"],\n",
    "            \"Anonymized R¬≤\": anon[\"test_r2\"],\n",
    "            \"R¬≤ Difference\": anon[\"test_r2\"] - orig[\"test_r2\"],\n",
    "            \"Original RMSE\": orig[\"test_rmse\"],\n",
    "            \"Anonymized RMSE\": anon[\"test_rmse\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nMean Absolute Error (MAE) - Lower is better:\")\n",
    "print(\n",
    "    comparison_df[\n",
    "        [\"Model\", \"Original MAE\", \"Anonymized MAE\", \"MAE % Change\"]\n",
    "    ].to_string(index=False)\n",
    ")\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(\"\\nR¬≤ Score - Higher is better:\")\n",
    "print(\n",
    "    comparison_df[[\"Model\", \"Original R¬≤\", \"Anonymized R¬≤\", \"R¬≤ Difference\"]].to_string(\n",
    "        index=False\n",
    "    )\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Summary statistics\n",
    "avg_mae_change = comparison_df[\"MAE % Change\"].mean()\n",
    "avg_r2_change = comparison_df[\"R¬≤ Difference\"].mean()\n",
    "\n",
    "print(f\"\\nüìä SUMMARY:\")\n",
    "print(f\"  Average MAE change: {avg_mae_change:+.2f}%\")\n",
    "print(f\"  Average R¬≤ change: {avg_r2_change:+.4f}\")\n",
    "\n",
    "if abs(avg_mae_change) < 5:\n",
    "    print(f\"  ‚úì Excellent utility preservation: Model performance nearly identical\")\n",
    "elif abs(avg_mae_change) < 10:\n",
    "    print(f\"  ‚úì Good utility preservation: Minimal impact on model performance\")\n",
    "elif abs(avg_mae_change) < 20:\n",
    "    print(f\"  ‚ö† Moderate utility loss: Some impact on model performance\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Significant utility loss: Consider less aggressive anonymization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dda5a4",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9579db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: MAE Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MAE Comparison Bar Chart\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(\n",
    "    x - width / 2, comparison_df[\"Original MAE\"], width, label=\"Original\", alpha=0.8\n",
    ")\n",
    "bars2 = ax1.bar(\n",
    "    x + width / 2, comparison_df[\"Anonymized MAE\"], width, label=\"Anonymized\", alpha=0.8\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Mean Absolute Error ($)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\n",
    "    \"Model Performance: Original vs Anonymized Data\\n(Lower is Better)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
    "ax1.legend()\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"${height:,.0f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "# R¬≤ Comparison\n",
    "ax2 = axes[1]\n",
    "bars3 = ax2.bar(\n",
    "    x - width / 2, comparison_df[\"Original R¬≤\"], width, label=\"Original\", alpha=0.8\n",
    ")\n",
    "bars4 = ax2.bar(\n",
    "    x + width / 2, comparison_df[\"Anonymized R¬≤\"], width, label=\"Anonymized\", alpha=0.8\n",
    ")\n",
    "\n",
    "ax2.set_xlabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"R¬≤ Score\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_title(\n",
    "    \"Model Performance: R¬≤ Score Comparison\\n(Higher is Better)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
    "ax2.legend()\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "ax2.set_ylim(\n",
    "    0,\n",
    "    max(comparison_df[\"Original R¬≤\"].max(), comparison_df[\"Anonymized R¬≤\"].max()) * 1.1,\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance comparison visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42225ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Performance Degradation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = [\n",
    "    \"green\" if x < 5 else \"orange\" if x < 15 else \"red\"\n",
    "    for x in comparison_df[\"MAE % Change\"].abs()\n",
    "]\n",
    "\n",
    "bars = ax.barh(\n",
    "    comparison_df[\"Model\"], comparison_df[\"MAE % Change\"], color=colors, alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"MAE % Change (Original ‚Üí Anonymized)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Impact of Anonymization on Model Performance\\n(Negative = Better Performance on Anonymized Data)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.axvline(x=0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, val) in enumerate(zip(bars, comparison_df[\"MAE % Change\"])):\n",
    "    ax.text(\n",
    "        val + (1 if val > 0 else -1),\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{val:+.2f}%\",\n",
    "        ha=\"left\" if val > 0 else \"right\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"green\", alpha=0.7, label=\"Excellent (<5% change)\"),\n",
    "    Patch(facecolor=\"orange\", alpha=0.7, label=\"Moderate (5-15% change)\"),\n",
    "    Patch(facecolor=\"red\", alpha=0.7, label=\"Significant (>15% change)\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance degradation visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Prediction Scatter Plots (Best Model)\n",
    "best_model_name = comparison_df.loc[comparison_df[\"Original R¬≤\"].idxmax(), \"Model\"]\n",
    "print(f\"Best performing model (by R¬≤): {best_model_name}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original predictions\n",
    "ax1 = axes[0]\n",
    "y_pred_orig = results_original[best_model_name][\"predictions\"]\n",
    "ax1.scatter(y_test_orig, y_pred_orig, alpha=0.5, s=30)\n",
    "ax1.plot(\n",
    "    [y_test_orig.min(), y_test_orig.max()],\n",
    "    [y_test_orig.min(), y_test_orig.max()],\n",
    "    \"r--\",\n",
    "    lw=2,\n",
    "    label=\"Perfect Prediction\",\n",
    ")\n",
    "ax1.set_xlabel(\"Actual Income ($)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Predicted Income ($)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\n",
    "    f'{best_model_name} - Original Data\\nR¬≤ = {results_original[best_model_name][\"test_r2\"]:.4f}',\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Anonymized predictions\n",
    "ax2 = axes[1]\n",
    "y_pred_anon = results_anonymized[best_model_name][\"predictions\"]\n",
    "ax2.scatter(y_test_anon, y_pred_anon, alpha=0.5, s=30, color=\"orange\")\n",
    "ax2.plot(\n",
    "    [y_test_anon.min(), y_test_anon.max()],\n",
    "    [y_test_anon.min(), y_test_anon.max()],\n",
    "    \"r--\",\n",
    "    lw=2,\n",
    "    label=\"Perfect Prediction\",\n",
    ")\n",
    "ax2.set_xlabel(\"Actual Income ($)\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Predicted Income ($)\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_title(\n",
    "    f'{best_model_name} - Anonymized Data\\nR¬≤ = {results_anonymized[best_model_name][\"test_r2\"]:.4f}',\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Prediction scatter plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Distribution Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Age distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(df[\"age\"], bins=30, alpha=0.5, label=\"Original\", density=True)\n",
    "ax1.hist(\n",
    "    df_anon_processed[\"age\"].dropna(),\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    label=\"Anonymized\",\n",
    "    density=True,\n",
    ")\n",
    "ax1.set_xlabel(\"Age\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Density\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\n",
    "    \"Age Distribution: Original vs Anonymized\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Zip prefix distribution\n",
    "ax2 = axes[0, 1]\n",
    "original_zip_counts = df[\"zip_prefix\"].value_counts().sort_index()\n",
    "anon_zip_counts = df_anon_processed[\"zip_prefix\"].value_counts().sort_index()\n",
    "x_labels = sorted(set(original_zip_counts.index) | set(anon_zip_counts.index))\n",
    "ax2.plot(\n",
    "    original_zip_counts.index,\n",
    "    original_zip_counts.values,\n",
    "    \"o-\",\n",
    "    label=\"Original\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.plot(\n",
    "    anon_zip_counts.index,\n",
    "    anon_zip_counts.values,\n",
    "    \"s-\",\n",
    "    label=\"Anonymized\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.set_xlabel(\"Zip Code Prefix\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Count\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_title(\n",
    "    \"Zip Code Distribution: Original vs Anonymized\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Income distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(df[\"income\"], bins=30, alpha=0.5, label=\"Original\", density=True)\n",
    "ax3.hist(\n",
    "    df_anon_processed[\"income\"].dropna(),\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    label=\"Anonymized\",\n",
    "    density=True,\n",
    ")\n",
    "ax3.set_xlabel(\"Income ($)\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_ylabel(\"Density\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_title(\n",
    "    \"Income Distribution: Original vs Anonymized\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Correlation heatmap comparison\n",
    "ax4 = axes[1, 1]\n",
    "corr_orig = df[[\"age\", \"zip_prefix\", \"income\"]].corr()\n",
    "corr_anon = df_anon_processed[[\"age\", \"zip_prefix\", \"income\"]].corr()\n",
    "corr_diff = corr_anon - corr_orig\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_diff,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    vmin=-0.1,\n",
    "    vmax=0.1,\n",
    "    ax=ax4,\n",
    "    cbar_kws={\"label\": \"Correlation Difference\"},\n",
    ")\n",
    "ax4.set_title(\n",
    "    \"Correlation Matrix Difference\\n(Anonymized - Original)\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Distribution comparisons visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7095e0",
   "metadata": {},
   "source": [
    "## 8. Statistical Utility Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f667202",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095010ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the utility metrics module for comprehensive analysis\n",
    "print(\"Calculating comprehensive utility metrics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare dataframes for utility comparison\n",
    "df_orig_compare = df[[\"age\", \"income\"]].copy()\n",
    "df_orig_compare[\"zip\"] = df[\"zip_prefix\"]\n",
    "\n",
    "df_anon_compare = df_anon_processed[[\"age\", \"income\"]].copy()\n",
    "df_anon_compare[\"zip\"] = df_anon_processed[\"zip_prefix\"]\n",
    "\n",
    "# Generate utility report\n",
    "try:\n",
    "    utility_report = compare_utility(df_orig_compare, df_anon_compare)\n",
    "    print(utility_report.get_summary())\n",
    "except Exception as e:\n",
    "    print(f\"Error generating utility report: {e}\")\n",
    "    print(\"\\nManual metrics calculation:\")\n",
    "\n",
    "    for col in [\"age\", \"income\", \"zip\"]:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        orig_mean = df_orig_compare[col].mean()\n",
    "        anon_mean = df_anon_compare[col].mean()\n",
    "        orig_std = df_orig_compare[col].std()\n",
    "        anon_std = df_anon_compare[col].std()\n",
    "\n",
    "        print(f\"  Original - Mean: {orig_mean:.2f}, Std: {orig_std:.2f}\")\n",
    "        print(f\"  Anonymized - Mean: {anon_mean:.2f}, Std: {anon_std:.2f}\")\n",
    "        print(\n",
    "            f\"  Mean difference: {abs(orig_mean - anon_mean):.2f} ({abs(orig_mean - anon_mean)/orig_mean*100:.2f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Std difference: {abs(orig_std - anon_std):.2f} ({abs(orig_std - anon_std)/orig_std*100:.2f}%)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bd76b",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0927c-bf95-4f08-95cb-d5337bcf210c",
   "metadata": {},
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"UTILITY VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìã DATASET INFORMATION:\")\n",
    "print(f\"  ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Features used: age, zip code prefix\")\n",
    "print(f\"  ‚Ä¢ Target variable: income\")\n",
    "print(f\"  ‚Ä¢ Train/test split: 80/20\")\n",
    "\n",
    "print(\"\\nüîê ANONYMIZATION APPLIED:\")\n",
    "print(f\"  ‚Ä¢ Age: Generalized to ranges (e.g., 30-39)\")\n",
    "print(f\"  ‚Ä¢ Zip code: Generalized to 3-digit prefixes\") \n",
    "print(f\"  ‚Ä¢ Income: Generalized to ranges\")\n",
    "\n",
    "print(\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Models tested: {len(models)}\")\n",
    "print(f\"  ‚Ä¢ Best model: {best_model_name}\")\n",
    "print(f\"  ‚Ä¢ Average MAE change: {avg_mae_change:+.2f}%\")\n",
    "print(f\"  ‚Ä¢ Average R¬≤ change: {avg_r2_change:+.4f}\")\n",
    "\n",
    "print(\"\\nüéØ KEY FINDINGS:\")\n",
    "\n",
    "# Calculate utility retention\n",
    "if abs(avg_mae_change) < 5:\n",
    "    utility_status = \"EXCELLENT\"\n",
    "    utility_emoji = \"‚úÖ\"\n",
    "    utility_desc = \"near-identical performance\"\n",
    "elif abs(avg_mae_change) < 10:\n",
    "    utility_status = \"GOOD\"\n",
    "    utility_emoji = \"‚úì\"\n",
    "    utility_desc = \"minimal performance degradation\"\n",
    "elif abs(avg_mae_change) < 20:\n",
    "    utility_status = \"MODERATE\"\n",
    "    utility_emoji = \"‚ö†\"\n",
    "    utility_desc = \"noticeable performance impact\"\n",
    "else:\n",
    "    utility_status = \"POOR\"\n",
    "    utility_emoji = \"‚ùå\"\n",
    "    utility_desc = \"significant performance loss\"\n",
    "\n",
    "print(f\"  {utility_emoji} Utility Preservation: {utility_status}\")\n",
    "print(f\"     Models show {utility_desc} on anonymized data\")\n",
    "\n",
    "# Best and worst performing models\n",
    "best_idx = comparison_df['MAE % Change'].abs().idxmin()\n",
    "worst_idx = comparison_df['MAE % Change'].abs().idxmax()\n",
    "\n",
    "print(f\"\\n  ‚Ä¢ Most resilient model: {comparison_df.loc[best_idx, 'Model']}\")\n",
    "print(f\"    ({comparison_df.loc[best_idx, 'MAE % Change']:+.2f}% MAE change)\")\n",
    "print(f\"\\n  ‚Ä¢ Most affected model: {comparison_df.loc[worst_idx, 'Model']}\")\n",
    "print(f\"    ({comparison_df.loc[worst_idx, 'MAE % Change']:+.2f}% MAE change)\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if abs(avg_mae_change) < 10:\n",
    "    print(\"  ‚úì Anonymization strategy is effective - good privacy/utility balance\")\n",
    "    print(\"  ‚úì Data suitable for machine learning and analytics\")\n",
    "    print(\"  ‚úì Consider this approach for similar use cases\")\n",
    "else:\n",
    "    print(\"  ‚ö† Consider less aggressive anonymization if higher utility is needed\")\n",
    "    print(\"  ‚ö† Evaluate if the privacy gains justify the utility loss\")\n",
    "    print(\"  ‚ö† May need different strategies for different columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Utility validation complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
